{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Preparation and Engineering for House Pricing\n",
    "\n",
    "In the previous exercise we have explored the famous Ames Housing data from Dean De Cock. We have seen that the dataset includes numerical and categorical features which also contain null values. Hence, the dataset is not yet ready to feed it to a Machine Learning model. This is the task of this exercise.\n",
    "\n",
    "As always, we start by **importing** the standard libraries **pandas** as pd and **numpy** as np, **loading** the dataset as a dataframe called **houses** and looking at the metadata of the dataset by using the **info()** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please import the dataframe **houses** from the path **'../data/houses.csv'**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Metadata\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, drop the column **'Id'** which will be irrelevant for the regression task since it will not help us to explain the target 'SalePrice'.\n",
    "\n",
    "To solve this task please use the **drop('Id')** method on the dataframe **houses**. Furthermore, set the arguments **errors** to **'ignore'** and **inplace** to **True**. Do not forget to use the **correct axis** argument. Afterwards, check if the column has been dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop irrelevant column id\n",
    "<FILL-IN>.drop(<FILL-IN>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print head\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe you have already noticed that a lot of columns contain NaN values. However, the Machine Learning Algorithms which we will use cannot handle NaN Values, hence we have to remove or fill them.\n",
    "\n",
    "\n",
    "First, let us **investigate the percentage of null values** in each column. Compute the amount of null/NaN values in each column. To solve this task you can use the **isnull()** and **sum()** method. In order to get the ratio you need to **divide by the number of rows**. Please name the resulting pandas series **null_series** and print it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the amount of null values in each column\n",
    "<FILL-IN> = <FILL-IN>\n",
    "null_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort the series in descending order by using the **sort_values()** method on the **null_series**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print sorted results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the columns contain a lot of null values (>15 %) and it is better to drop these instead of filling the nulls with some other values. For this reason, create a list called **dropCols** by **slicing**(conditional indexing) the pandas series so that it only contains the entries where the **ratio > 0.15**, accessing the attribute **index** from the resulting series and using the method **tolist()** on that result. Print the list **dropCols**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list containing column names of columns with more than 15 % null values\n",
    "<FILL-IN> = <FILL-IN>\n",
    "dropCols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, **drop** all the columns which are contained in the list **dropCols** from the houses dataframe by using the **drop()** method. Please, set again the correct **axis**, **errors** to **'ignore'** and **inplace** to **True**. Afterwards, check if the columns have been dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop cols\n",
    "<FILL-IN>\n",
    "houses.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical and numerical columns will be treated differently. For this reason, we need to split the dataframe into one containing only the numerical attributes and another one containing only the categorical ones.\n",
    "\n",
    "To solve this task you can use the method **select_dtypes()** on the **houses** dataframe. Please, name the resulting dataframe, only containing object data types, **houses_cat** and the other one **houses_num**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data in categorical and numerical\n",
    "<FILL-IN> = houses.select_dtypes(include=[object])\n",
    "<FILL-IN> = <FILL-IN>.select_dtypes(exclude=[object])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treatment of Categorical Features\n",
    "The categorical features need to be **transformed into numerical ones** in order to work with Sklearn Machine Learning Models.\n",
    "Therefore, we will use the Dummy Encoding method **OneHotEncoding**. This will result to (k-1) new columns for an attribute with k different categories. Hence, we introduce a lot of **new dimensions** to the dataframe. For this reason it is useful to first **check the cardinality** of the categorical features.\n",
    "\n",
    "Please, use the method **nunique()** on the **houses_cat** dataframe and sort the result in ascending order by using the method **sort_values()**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the cardinality of the categorical features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature with the highest cardinality is **Neighborhood** and contains **25** different categories. This is not too much and we don't have to drop or modify some of these columns.\n",
    "\n",
    "**Remark and Bonus**: Actually, it is also important to look at the frequencies of the categories. Maybe one category appears only once and hence is not of much value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the next step we fill the Null values of these features. Here, we use a very simple approach where we fill each NaN value with the string 'Unknown'. Therefore, use the method **fillna('Unknown')** on the dataframe **houses_cat** and give the resulting dataframe again the name **houses_cat**. Afterwards, use the method **info()** on the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN\n",
    "houses_cat = <FILL-IN>\n",
    "<FILL-IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Side remark**:\n",
    "The column type of the categorical feature is object, which is actually a kind of a string data type in pandas. We can transform it to an actual category type (similar to the factor type in R). This data type will occupy less memory.\n",
    "\n",
    "Please **execute the cell** below and **compare the memory** usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform to categorical values\n",
    "houses_cat = houses_cat.apply(lambda x: x.astype('category'))\n",
    "<FILL-IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final preparation step for the categorical data is **OneHotEncoding**. An easy way to do that for all columns at once is by using the **get_dummies()** function of pandas. Please, use that function and set the argument **data** to the dataframe **houses_cat**, **prefix_sep** to the **equal sign** and **drop_first** to **True**. Call the resulting dataframe **houses_cat_dum**. Finally, check the number of columns and their data type contained in the new dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummies\n",
    "<FILL-IN> = <FILL-IN>\n",
    "<FILL-IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are done with the treatment of the categorical variables. Before we proceed we **rejoin** the transformed categorical dataframe **houses_cat_dum** with the numerical one **houses_num** by using the **join()** method. Please, give the new dataframe the name **houses_prep** and apply the info method to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rejoin\n",
    "<FILL-IN> = <FILL-IN>\n",
    "<FILL-IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treatment of numerical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we fill the NaN values of the remaining features by the median of the corresponding column. The computation of the median requires a **fit** method. Every preprocessing step which requires such a method **must be fit only on the training data**. Hence, we split our dataset into a **training** and **test** dataset called **houses_train** and **houses_test**. The houses_test data set will be used to evaluate our predictions at the very end.\n",
    "\n",
    "Please, **import** the function **train_test_split** from the **sklearn.model_selection** and apply it. As the first argument use the dataframe **houses_prep**. The second argument should be set to **test_size=0.2**, i.e. we use 20% of the data as a test data set. In order to get the same split for all participants we set the **random seed** to the value 42. This allows us to get identical training and test datasets. Therefore, please use as a third agument of the function **random_state=42**. The function returns a list containing two dataframes which can be unpacked directly.\n",
    "\n",
    "**Remark**: Please look at the docstring of the function. What do you notice when you look at the function arguments? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from <FILL-IN> import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataframe in training and test dfs\n",
    "houses_train, <FILL-IN> = train_test_split(<FILL-IN>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fill the NaN values we create an **imputer object** of the preprocessing class **SimpleImputer**. Hence, **import** the class from the module **sklearn.impute**, instanciate it using **strategy='median'** as the argument and give the resulting object the name **imputer**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Imputer\n",
    "from <FILL-IN> import <FILL-IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciate an Imputer object called imputer\n",
    "<FILL-IN> = <FILL-IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As most of the Sklearn Machine Learning Models and preprocessing classes the class Imputer contains the methods fit, transform and fit_transform. Here, the method fit computes the median of the columns and stores it inside the object, transform fills the NaN values by the stored medians/values and fit_transform combines both steps.\n",
    "\n",
    "Please, use the **fit_transform** method of the imputer object. Use the dataframe **houses_train** as the argument. Call the result **train_array**. Afterwards, use **only the transform method** of the imputer on **houses_test** and call the result test_array. Print both results. What do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and use imputer\n",
    "<FILL-IN> = <FILL-IN>.fit_transform(<FILL-IN>)\n",
    "<FILL-IN> = imputer.transform(houses_test)\n",
    "<FILL-IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hey, we lost the column and index names of our data. This is due to the reason that Sklearn objects accept pandas dataframes as input, but return (in general) numpy arrays. Sklearn uses numpy arrays and not pandas dataframes as central data types. However, sometimes it is more feasible to work with dataframes. Therefore, we **create** again dataframes called **train_df** and **test_df** out of the two numpy arrays. We can get the **column names** and **indices** from the two dataframes **houses_train** and **houses_test** by accessing the **attributes columns** and **index** and transform the results to a list by using the method **tolist()**. Since the columns are the same in both dataframes we only have to extract them from one of the dataframes. However, the **indices** differ and need to be extracted **separately**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract columns and indices\n",
    "cols = <FILL-IN>\n",
    "train_ind = <FILL-IN>\n",
    "test_ind = <FILL-IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can create a dataframe by using the pandas method **DataFrame()**. As the arguments use the arrays, columns and indices. Call the resulting dataframes **train_df** and **test_df**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate dataframes\n",
    "train_df = pd.DataFrame(<FILL-IN>)\n",
    "<FILL-IN> = <FILL-IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before saving the data perform a quick check to see if there are still null values in the dataframes and if all the data types are numerical. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final check (Nulls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final check (data types)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we **save the two dataframes** (train_df, test_df) in a python specific binary format called **pickle**. This can be done by using the method **.to_pickle()** on the dataframes. As the argument please use **'houses_train.pkl'** and **'houses_test.pkl'** as the file path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the two dataframes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the end of the exercise."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
