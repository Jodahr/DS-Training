{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression models for House Pricing\n",
    "In the last exercise we have prepared our dataset so that we can feed it to a machine learning model for regression. Building such a regression model will be the task of this exercise.\n",
    "\n",
    "First, we **import** the standard libraries **numpy** as np and **pandas** as pd. Afterwards, we **read the pickled data** from the previous exercise. Therefore, use the Pandas method **read_pickle()** with the filepathes **'../data/houses_train.pkl'** and **'../data/houses_test.pkl'** as the argument. Call the resulting dataframes **train** and **test**.\n",
    "\n",
    "**Remark**: If the pickle files are not working, please use the csv files together with the method *.read_csv()*. Please set the optional argument *index_col* to 0 for the csv method. This will set the first column of the csv file to the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-02T19:01:56.298807Z",
     "start_time": "2018-05-02T19:01:55.868413Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-02T19:01:57.301659Z",
     "start_time": "2018-05-02T19:01:57.260834Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load dataframes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we train a model let us **separate** the **features** and the **target**. Hence, **create** variables called **X_train** and **X_test** which include all the data of the features from the training and test data, respectively, and variables called **y_train**, and **y_test** which contain only the target ('SalePrice')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-02T19:02:00.910587Z",
     "start_time": "2018-05-02T19:02:00.888670Z"
    }
   },
   "outputs": [],
   "source": [
    "# Separate features and target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Tree\n",
    "Finally, we can train our first model: a regression tree. For this reason we import the following class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-02T19:02:04.317730Z",
     "start_time": "2018-05-02T19:02:03.921247Z"
    }
   },
   "outputs": [],
   "source": [
    "# Just execute\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please **create an object of that class called tree_reg**. As the argument of the constructor use **random_state=42**. For all the other parameters we use the default values. If you are interested in the possible arguments and default settings you can use 'Shift+Tab' or you may have a look at http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-02T19:02:05.653245Z",
     "start_time": "2018-05-02T19:02:05.649066Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create object of class DecisionTreeRegressor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, use the **fit(x,y)** method of the **tree_reg** object to train the model. As the argument use the training dataset **(X_train, y_train)**. The return value of that method is again the decision tree object, but this time with fitted model parameters. You do not need to assign the result to a new variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-02T19:02:07.615375Z",
     "start_time": "2018-05-02T19:02:07.572593Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of a transform method like for the preprocessing objects (e.g. Imputer), machine learning models have **predict(X) methods**. For further details please have a look at the documentation.\n",
    "\n",
    "Next, we want to use that predict method to compute predictions for house prices on our test dataset. Hence, please use the **predict** method of the **tree_reg** object on the test data **X_test** and save the result to a variable called **y_pred**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-02T19:02:09.271649Z",
     "start_time": "2018-05-02T19:02:09.264687Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compute predictions for the test data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The return value is again a numpy array. We want to **compare** these values **to the true house prices** of the test dataset. For this reason, **construct** a dataframe called **results** which contains two columns: the true y values and the predictions. Call the two columns **'y_true'** and **'y_pred'**, respectively. Afterwards, print the first 10 values of the dataframe.\n",
    "\n",
    "**Hint**: An easy way to do that is by using the *dictionary*-method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-02T19:02:11.668833Z",
     "start_time": "2018-05-02T19:02:11.660333Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create result dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of our Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At a first glance our results do not look that bad. Of course, we want to quantify the performance of the regression model by using a **performance metric**. One common metric, which has also been used by the construction of the regression tree, is the **mean squared error** (mse) which is the squared sum of the residuals. Please, perform the following steps to compute the mse: first, **add a new column called 'residuals'** which contains the residuals (**differences between y_true and y_pred**) to the results dataframe. Afterwards, take the **mean of the squared residuals** by using the mean function of numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-02T19:02:14.025645Z",
     "start_time": "2018-05-02T19:02:13.933611Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compute residuals and mse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we do not have to compute that metric manually every time. **Sklearn provides a lot of different metrics** for regression, classification and clustering. Please, **import** the function **mean_squared_error** from the module **sklearn.metrics**. Afterwards, use that function to **crosscheck your result**. Furthermore, take the square root of the result. This gives the **root mean squared error** (rmse). It represents the sample standard deviation of the differences between predicted values and observed values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-02T19:02:15.571502Z",
     "start_time": "2018-05-02T19:02:15.569320Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import mse function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-02T19:02:16.013896Z",
     "start_time": "2018-05-02T19:02:16.000793Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compute mse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, you got the same result.\n",
    "\n",
    "Next, let us **plot the predictions** against the true house price values. Therefore, you can use the **seaborn** function **lmplot**. As the arguments use **data=results** and **height=8** and set **x** and **y** to **'y_pred'** and **'y_true'**, respectively. However, do not forget to **import the module first** and to issue the command **%matplotlib inline** afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-02T19:04:00.985401Z",
     "start_time": "2018-05-02T19:04:00.724786Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import seaborn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lmplot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results do not look that bad. Of course, we can see **a few outliers** which are always hard to predict. For a perfect model all the points would lie on the angle bisector.\n",
    "\n",
    "Next, we look at the **distribution of the residuals**. Ideally, the residuals should be **normally distributed** around zero. If you see a completely different shape you have used a model which does not describe the data very well and which is conceptually wrong. Can you think of an example?\n",
    "\n",
    "Please, use the function **distplot** of seaborn to look at the distribution of the residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-02T19:14:52.437294Z",
     "start_time": "2018-05-02T19:14:52.254490Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot distribution of the residuals\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks a bit like a normal distribution. However, we have large residuals for some outliers. Furthermore, the distribution is slighly right skewed. Do you have any idea why this could be the case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the theory part we have seen another performance measure for regression, the **r2 score**, which can be seen as the proportion of **explained variance** of the target with our model.\n",
    "\n",
    "**Bonus:**\n",
    "\n",
    "First, compute this score by hand. Therefore, compute the **sum of all squared residuals** and **divide it by the total sum of squares (proportional to the variance of y_true)**. Afterwards, **subtract the result from the value one**. Crosscheck your result with the **r2_score** function from the modul **sklearn.metrics**. The formula is given by\n",
    "\n",
    "$$ r^2 = 1 - \\frac{\\sum_i residuals_i^2}{\\sum_i(y_{true,i}- \\bar{y}_{true})^2}, $$\n",
    "\n",
    "where $\\bar{y}_{true}$ represents the mean value of the observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-02T19:24:05.336414Z",
     "start_time": "2018-05-02T19:24:05.319211Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compute r2 score by hand\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-02T19:23:45.826136Z",
     "start_time": "2018-05-02T19:23:45.818677Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compute r2_score with sklearn function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, the mean squared error is not the best metric for our target variable and although our model uses this measure during training. **Can you think of any problem with the mean squared error regarding the distribution and scales of our target**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe you came up with the actual problem. If not, we will explain that later during the discussion of the exercise. A possible solution to overcome this problem is taking the log of the house prices and using it as the new target.\n",
    "\n",
    "Please, **create a new target variable** **y_test_log** and **y_train_log**. Afterwards, **retrain the model** and compute again the **lmplot** and the **r2_score** with the log data. \n",
    "\n",
    "\n",
    "**Hint**: Use np.log() and np.exp() for the transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-02T19:37:09.976401Z",
     "start_time": "2018-05-02T19:37:09.972408Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compute the log of the target variable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train another model called **tree_reg_log** using the logarithmic target.\n",
    "\n",
    "**Remark**: Instanciate a new DecisonTreeRegressor. Set the random_state again to 42."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-02T19:42:57.411845Z",
     "start_time": "2018-05-02T19:42:57.375734Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train a new model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the logarithmic predictions using the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-02T19:43:24.141429Z",
     "start_time": "2018-05-02T19:43:24.133843Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compute log predictions on test data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add **two more columns** to the result dataframe containing the **logarithmic predictions** and **true logarithmic house price values**. Call the columns 'y_true_log' and 'y_pred_log'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-02T19:46:06.882775Z",
     "start_time": "2018-05-02T19:46:06.878301Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add result columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-02T19:48:33.953352Z",
     "start_time": "2018-05-02T19:48:33.947668Z"
    }
   },
   "outputs": [],
   "source": [
    "# log r2_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bonus**:\n",
    "\n",
    "Compute the r2_score, the mse and rmse on the backtransformation of the log predictions and log observations.\n",
    "\n",
    "**Hint**: Use the inverse transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original scale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mse and rmse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use lmplot to plot the log precitions against the true values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-02T19:52:30.063175Z",
     "start_time": "2018-05-02T19:52:29.763539Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create lmplot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bonus**: \n",
    "\n",
    "Plot the distribution of the logarithmic residuals and of the back transformed residuals.\n",
    "\n",
    "Furthermore, do the following: \n",
    "\n",
    "Think of a regression model with a single feature X and a target label y. If you train a regression tree, how many dimensions does the hyperrectangle have? To solve this task, please draw a scatter plot with a linear relationship between X and y. Furthermore, draw (in a qualitative way) four hyperrectangles into the graph and the regression predictions of the regression tree. In addition, draw a regression line of a linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-02T19:54:40.874014Z",
     "start_time": "2018-05-02T19:54:40.740495Z"
    }
   },
   "outputs": [],
   "source": [
    "# Log residuals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-02T19:55:24.091310Z",
     "start_time": "2018-05-02T19:55:23.944065Z"
    }
   },
   "outputs": [],
   "source": [
    "# Backtransformed residuals\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of the regression tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we want to visualize a regression tree. Since the previously trained trees have been built completely and not been pruned, the resulting plot would be way too large. Hence, we train another tree by setting the hyperparameter **max_depth** to **4**. Please create such a tree. Call the object **reg_tree_fixedDepth** and set the **random_state to 42**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-02T20:19:07.082655Z",
     "start_time": "2018-05-02T20:19:07.080565Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instanciate a tree with fixed max depth\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the new tree on the log data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-02T20:19:07.797546Z",
     "start_time": "2018-05-02T20:19:07.782202Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train tree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the tree we have to **import** the module **export_graphviz** **from** the module **sklearn.tree**. Please do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-02T20:19:08.408500Z",
     "start_time": "2018-05-02T20:19:08.406373Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import export_graphiz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To export the tree, just execute the command below. Make sure that your tree object has exactly the same variable name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-02T20:19:08.912433Z",
     "start_time": "2018-05-02T20:19:08.905675Z"
    }
   },
   "outputs": [],
   "source": [
    "# Just execute\n",
    "export_graphviz(reg_tree_fixedDepth, out_file=\"tree_4_reg.dot\",\n",
    "                feature_names=X_train.columns.tolist(),\n",
    "                filled=True, rounded=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we have to render the *tree_4_reg.dot* file. Therefore, we have to install another package. Open a terminal and execute the command **sudo apt install graphviz**. If the package has been installed, you can execute the command in the cell below. Afterwards, you should find an image of the tree in your jupyter home folder.\n",
    "\n",
    "**Remark:** If you cannot install the package, you can copy the content of the *tree_4_reg.dot* file to a web application http://webgraphviz.com/.\n",
    "\n",
    "**Can you describe the decision tree?**\n",
    "\n",
    "**Bonus**: Please try to recompute some of the *samples* and *values* for the first two levels. Remember, that the tree shows values of the training dataset and not the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "dot -Tpng tree_4_reg.dot -o tree_4_reg.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First level\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second level (left)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second level (right)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-02T20:33:40.171126Z",
     "start_time": "2018-05-02T20:33:40.169248Z"
    }
   },
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the theory part we have already learned that a single tree overfits the data pretty often. Hence, an ensemble of uncorrelated trees like the random forest could be a better choice.\n",
    "\n",
    "Please **import** the **RandomForestRegressor** from the module **sklearn.ensemble** and create an object **rf_reg**. Afterwards, train the model on the log data and compute the r2 score on the log data.\n",
    "\n",
    "**Remark**: Do not forget to set the **random_state** again to **42**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-02T20:42:14.863725Z",
     "start_time": "2018-05-02T20:42:14.860799Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import Random Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-02T20:42:24.410469Z",
     "start_time": "2018-05-02T20:42:24.404946Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create object rf_reg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-02T20:42:54.045705Z",
     "start_time": "2018-05-02T20:42:53.845112Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train model on log data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-02T20:44:46.122718Z",
     "start_time": "2018-05-02T20:44:46.114604Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compute r2 score on log data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hey, this is much better. Maybe we can even improve upon this result by adding more trees to the forest? Please, create **another RandomForest** and increase the **number of trees** to **n_estimators=100** and set the **random_state** again to **42**. After the **model training** please compute again the **r2 score**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-02T20:52:21.689696Z",
     "start_time": "2018-05-02T20:52:03.181456Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create random forest with more trees and retrain and reevaluate the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hm, not much better, but at least a bit. Adding more trees never hurts, except the computation performance. However, at some point more trees do not improve the model at all.\n",
    "\n",
    "### Feature Importance\n",
    "\n",
    "Now, let us go on to compute the **feature importance**. The most important feature is the one which has reduced the mean squared error among all the splits. The random forest (and also the decision tree) has an **attribute** called **feature\\_importances\\_**.\n",
    "\n",
    "Please use that attribute and give the extracted result a new name called **feature\\_importances**. Afterwards, print the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-02T20:56:41.547099Z",
     "start_time": "2018-05-02T20:56:41.490459Z"
    }
   },
   "outputs": [],
   "source": [
    "# Extract feature importance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is an array which contains the proportion of the reduced mean squared error for each feature. Hence, summing over all values yields one. But which element corresponds to which feature? Fortunately, the order is the same as the column order in the feature dataframe X_train. Therefore, **extract the feature names** by accessing the **attribute columns** and transform it to a list by using the method **tolist()**. Call that list **features**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-02T21:01:12.598812Z",
     "start_time": "2018-05-02T21:01:12.595852Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create feature list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to combine the feature names and the feature importances. There are several methods to do that.\n",
    "\n",
    "One solution is creating a dataframe with those two columns:\n",
    "\n",
    "Create a **dataframe** called **importance_df** using the *dictionary method* (pd.DataFrame(dict)). As keys of the dict use **'feature'** and **'importance'**, as the data use the arrays/lists **features** and **feature_importances**, respectively. Finally, **sort the dataframe** by the importance value in descending order and print the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-02T21:07:26.566911Z",
     "start_time": "2018-05-02T21:07:26.548489Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create dataframe and sort it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we only see the impact of single categories of the categorical variables.\n",
    "\n",
    "**Bonus**: Can you sum all the values belonging to one categorical variable?\n",
    "\n",
    "**Hint**:\n",
    "\n",
    "1. Use apply method and the split function on the column importance_df[feature]\n",
    "2. Use groupby, sum, and sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-02T21:13:19.420816Z",
     "start_time": "2018-05-02T21:13:19.417714Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create new column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-02T21:18:07.508816Z",
     "start_time": "2018-05-02T21:18:07.498505Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compute sum\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This is the end of the exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus I: Linear Regression\n",
    "Compute a linear Regression model. Therefore, load the model **LinearRegression** from the module **sklearn.linear_model**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-02T21:23:19.952856Z",
     "start_time": "2018-05-02T21:23:19.947888Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import model and create instance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-02T21:24:25.364490Z",
     "start_time": "2018-05-02T21:24:25.309202Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-02T21:24:25.588669Z",
     "start_time": "2018-05-02T21:24:25.578718Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compute predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-02T21:24:39.107137Z",
     "start_time": "2018-05-02T21:24:39.098944Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compute r2_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-02T21:25:30.271889Z",
     "start_time": "2018-05-02T21:25:30.268805Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add prediction to result df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-02T21:27:13.739476Z",
     "start_time": "2018-05-02T21:27:13.505933Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-02T21:27:25.180760Z",
     "start_time": "2018-05-02T21:27:25.055726Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot residuals\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus II: Linear Regression with Lasso regularization\n",
    "Compute a linear Regression model which includes a regularization term. You can think of such a term as a penalty term which disfavors many features and therefore sets some coefficients to zero if the features are not very useful.\n",
    "\n",
    "Hence, do similar steps as in Bonus I. The model is called Lasso and can be found in the module sklearn.linear_model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
