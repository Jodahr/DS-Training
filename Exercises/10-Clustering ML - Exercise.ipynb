{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering with Spark - Customer Segmentation\n",
    "In the previous exercise we have tried to get familiar to spark sql and the bills dataset. Now, we want to cluster the clients with respect to their shopping behaviour by using the KMeans algorithm of the Spark ML library.\n",
    "\n",
    "As before we need to import the needed modules and **create a SparkSession**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SparkSession\n",
    "from pyspark.sql import <FILL-IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create spark object\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the dataframe **bills_exploded_parquet**, which we have created in the previous exercise. Call the dataframe **bills_exploded**. Afterwards, **cache** the dataframe by using the cache method and trigger the caching by applying the count method on the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame bills\n",
    "<FILL-IN> = spark.read.parquet(<FILL-IN>)\n",
    "<FILL_IN>.cache()\n",
    "\n",
    "print(<FILL-IN>.count())\n",
    "<FILL-IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to **count all products per customerId and category** and **all prices per customerId and category**. Therefore, you can either use the **groupBy('column1', 'column2')** method followed by the method **agg({'columnA': 'functionA', 'columnB': 'functionB'})** or you can **register** the dataframe bills_exploded as a **temporary table** by issuing the command **DataFrame.registerTempTable(\"tableName\")** and solve the task with **spark.sql(\"SQL-Code\")**. Finally, **rename the columns** sum(price) and count(category) to pricePerCategory and itemsPerCategory, respectively, by using the method **DataFrame.withColumnRenamed(\"oldName\", \"newName\")**. Please name your final dataframe **bills_aggredated**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Solution 1 ###\n",
    "\n",
    "bills_aggregated = bills_exploded.<FILL-IN>(\"customerId\", <FILL-IN>)\\\n",
    "    .agg({<FILL-IN>: \"count\", \"price\": <FILL-IN>})\\\n",
    "    .withColumnRenamed(<FILL-IN>, \"pricePerCategory\")\\\n",
    "    .withColumnRenamed(\"count(category)\", <FILL-IN>)\n",
    "\n",
    "\n",
    "### Solution 2 ###\n",
    "\n",
    "## Register Temp Table\n",
    "#billsProducts.registerTempTable(<FILL IN>)\n",
    "#billsAggregated = sqlContext.sql(<FILL IN>)\n",
    "\n",
    "# Cachen Dataframe\n",
    "bills_aggregated.cache()\n",
    "\n",
    "# Print first twenty rows\n",
    "bills_aggregated.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to extract a list of all available categories in the dataframe. Please, **select** the **category** and extract the unique elements. Afterwards, access the underlying rdd attribute of the dataframe, use a flatMap to flatten the rdd elements and apply the collect method to get a list.\n",
    "\n",
    "**Hint**: You can use the method *distinct()* (in Pandas it was unique())."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of categories to speed up pivot (see below)\n",
    "categoryList = bills_aggregated.<FILL-IN>('category').<FILL-IN>.rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "categoryList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is a bit more complicated. We want to transform each category to a column. As the column values we want to use the number of purchased items per customer. Therefore, we can use the **pivot** method. A pivot is an aggregation for which different entries of a grouped column will be transposed into separate columns. This method is very useful for data analysis. If you want to know more about pivot methods you can find further details here [Pivot-DataBricks](https://databricks.com/blog/2016/02/09/reshaping-data-with-pivot-in-apache-spark.html).\n",
    "\n",
    "**Remark**: *pivot* can only be applied after a *groupBy*. Afterwards, you have to perform an aggregation, e.g. with *agg*. In our case the sum only contains one element, since for each product category there is only one entry per customerId in the dataframe. We could have combined the pivot and the previous aggregation. Since some customers do not buy products from every category we get null values, which we fill with 0. Finally, we cache the dataframe.\n",
    "By the way, the previously created list helps us to speed up the execution of the pivot command. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pivot\n",
    "bills_agg_trans = bills_aggregated.groupBy(\"customerId\")\\\n",
    "    .pivot(\"category\").sum(\"itemsPerCategory\")\\\n",
    "    .na.fill(0).cache()\n",
    "\n",
    "# print customerId, BACKWAREN and MOLKE_EI\n",
    "bills_agg_trans.select(\"customerId\", \"BACKWAREN\", \"MOLKE_EI\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combination of the previous two steps\n",
    "bills_agg_trans_alt = bills_exploded.groupBy(\"customerId\")\\\n",
    "    .pivot(\"category\").agg({\"category\": \"count\"})\\\n",
    "    .na.fill(0).cache()\n",
    "    \n",
    "bills_agg_trans_alt.select(\"customerId\", \"BACKWAREN\", \"MOLKE_EI\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please **print the Schema** of the bills_agg_trans dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print schema\n",
    "<FILL-IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the total price and total number of shoppings per customer. Therefore, you can use the **bills_exploded** dataframe and the methods **groupBy** and **agg**. Please, rename the columns to **total_shoppings** and **total_price**. Afterwards, cache the dataframe and trigger the caching via a count method. The final dataframe should be called **shoppings_df**.\n",
    "\n",
    "**Hint**: For the aggregation you can either use agg({...}) followed by withColumnRenamed, or you can use agg(F.sum(column).alias('newName'), F.count()....) to directly rename the column inside the aggregate method. If you want you can also use the SQL solution. Don't forget to register the table first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# compute total price and total number of shoppings per customer\n",
    "<FILL-IN> = bills_exploded.groupBy(<FILL-IN>).agg(F.count(\"customerId\").alias(<FILL-IN>),\n",
    "                                             F.sum(<FILL-IN>).alias('total_price')).cache()\n",
    "shoppings_df.count()\n",
    "shoppings_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join the **shoppings_df** dataframe with the **bills_agg_trans** dataframe by using the join method like **df1.join(df2, on='column')**. Please join the two dataframes over the column **customerId**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join\n",
    "df = shoppings_df.<FILL-IN>.cache()\n",
    "df.count()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print schema\n",
    "df.<FILL-IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction and Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have modified our dataset so that we can start with feature extraction and preparation. For the clustering algorithm we want to use all product categories and the total price as features.\n",
    "\n",
    "Before we proceeed we import the following modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark ML\n",
    "from pyspark.ml.feature import VectorAssembler, Normalizer, MinMaxScaler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import SQLTransformer\n",
    "\n",
    "# Spark MLlib\n",
    "from pyspark.mllib.linalg import Vectors, VectorUDT\n",
    "\n",
    "# Matplotlib, Pandas and Numpy\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "plt.style.use('seaborn-pastel')\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Assembler\n",
    "\n",
    "To combine product category columns to a feature vector we can use the **VectorAssembler** Class. Please create an object of that class called **categoryAssembler**. Please set the constructor agument **inputCols** to the list of all categories, i.e. **categoryList**, and the outputCol to **\"productCategoryFeatures\"**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine product category columns\n",
    "categoryAssembler = <FILL-IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want, you can try out the transform method of the vectorAssembler object. As the input of the method please use the dataframe df. Later on we will combine all feature extraction and preparation steps to a so called machine learning pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<FILL-IN>.transform(df).select('productCategoryFeatures').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing the Feature Vector\n",
    "The generated feature vector contains the total amount of products per category which the customer has bought during all his shoppings. Since KMeans uses distances we would cluster clients which shop more often than others together. However, this is not what we want. Hence, we normalize the feature vector so that we get the proportion of items per category that a client has bought during all his shoppings. This can be achieved by using a normalizer which uses the L1 norm to our vector.\n",
    "\n",
    "Example:\n",
    "\n",
    "Imagine a client has bought 10 Milchprodukte, 5 Fleischprodukte and 15 Obstprodukte. The normalizer would give us the following vector:\n",
    "\n",
    "$\\vec{v} = [10,5,15] \\rightarrow |\\vec{v}|_{L_1} = 10 + 5 + 15 = 30 $  \n",
    "\n",
    "$\\vec{v}_{norm} = \\vec{v}/30 = [0.33,0.167,0.5]$\n",
    "\n",
    "To use the normalizer please create an object of the class Normalizer and set the arguments inputCol to productCategoryFeatures, outputCol to normCategoryFeatures and p to 1.0. Call the object normalizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizer using L_1 Norm (normalize by sum)\n",
    "<FILL-IN> = Normalizer(inputCol=<FILL-IN>, outputCol=\"normCategoryFeatures\",\n",
    "                        <FILL-IN>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom SQL Transformer and MinMax Scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to construct another feature: the average price per shopping of a client. Therefore, we use the SQLTransformer class. Please just execute the statement in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom sql transformer\n",
    "sqlTrans = SQLTransformer(\n",
    "    statement=\"SELECT *, total_price / total_shoppings as norm_price FROM __THIS__\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create another feature vector we construct again a VectorAssembler object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform price feature to vector; read and execute\n",
    "priceAssembler = VectorAssembler(inputCols=['norm_price'],\n",
    "                                  outputCol=\"normPriceFeature\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this feature is on a different scale than the previously created features we have to scale/standardize it. Please **create an object called scaler of the MinMaxScaler** class. As the parameters use **inputCol=\"normPriceFeature\" and outputcol=\"scaledPrice\"**. \n",
    "\n",
    "This scaler does the following:\n",
    "\n",
    "Assume the column contains the values [5,10,3,4]. The max is 10 and the min is 3. All elements will be transformed by using the formula:\n",
    "\n",
    "$$ z_i = \\frac{x_i - min}{max - min}$$\n",
    "\n",
    "and hence the range of the feature is between 0 and 1.\n",
    "\n",
    "**Remark**: Scaling along a column is called Standarizing and along a row or a feature vector is called normalizing. Often people do not strictly use this differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minMax scaler to scale column to get similar distances (range[0,1])\n",
    "scaler = MinMaxScaler(<FILL-IN>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we combine all features with another vector assembler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all feature columns; read and execute\n",
    "assembler = VectorAssembler(inputCols=['normCategoryFeatures', 'scaledPrice'],\n",
    "                                  outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMeans Model\n",
    "As the next step we need to instanciate the KMeans model. Hence, **create an object of the KMeans class called kmeans**. As the **k** (the number of clusters) use the number **4**, as the featuresCol use \"features\" and as the predictionCol use \"cluster\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clustering model\n",
    "kmeans = KMeans(<FILL-IN>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline\n",
    "\n",
    "Next, we create a machine learning pipeline. All previously defined transformations will be automatically applied if we pass the dataframe to the fit method of the pipeline.\n",
    "\n",
    "As the stages argument please pass a list of all needed steps in the correct order. The elements are:\n",
    "**kmeans, scaler, priceAssembler, sqlTrans, categoryAssembler, assembler, normalizer**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build pipeline\n",
    "pipeline = Pipeline(stages=[<FILL-IN>])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the fit method of the pipeline on the dataframe df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "model = pipeline.<FILL-IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can make predictions. Therefore, we can use the transform method of the model. This yields a new dataframe. Please call the result predictions. Cache the dataframe and trigger the caching via the count method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "predictions = model.<FILL-IN>(df)\n",
    "predictions.<FILL-IN>.count()\n",
    "predictions.select('customerId', 'features', 'cluster').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extract the cluster centroids from the kmeans model. Therefore, we have to extract the model from the pipeline. Please just read and execute the cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get cluster centers\n",
    "\n",
    "# extract kmeans model\n",
    "kmeansModel = model.stages[-1]\n",
    "\n",
    "# extract centers\n",
    "centers = kmeansModel.clusterCenters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TICKS ARE WRONG\n",
    "# barplot with cluster centers\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (25, 8)\n",
    "centers\n",
    "ind = np.arange(14)  # the x locations for the groups\n",
    "width = 0.2   \n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(ind, (centers[0][0:14]), width, color='r')\n",
    "rects2 = ax.bar(ind + width, (centers[1][0:14]), width, color='y')\n",
    "rects3 = ax.bar(ind + 2*width, (centers[2][0:14]), width, color='b')\n",
    "rects4 = ax.bar(ind + 3*width, (centers[3][0:14]), width, color='g')\n",
    "\n",
    "ax.set_ylabel('Anteil')\n",
    "ax.set_title('Warenanteil pro Cluster')\n",
    "ax.set_xticks(ind + width)\n",
    "ax.set_xticklabels(categoryList, rotation=45)\n",
    "\n",
    "\n",
    "ax.legend((rects1[0], rects2[0], rects3[0], rects4[0]), ('Cluster 0', 'Cluster 1','Cluster 2','Cluster 3'))\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe the plot above.  \n",
    "\n",
    "We can also compute the total revenue of the clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total revenue\n",
    "total_revenue = predictions.agg(F.sum('total_price')).rdd.flatMap(lambda x: x).collect()[0]\n",
    "\n",
    "# revenue per cluster\n",
    "predictions.groupBy('cluster').agg(F.count('*').alias('number_of_customers'),\n",
    "                                   (F.sum('total_price')/total_revenue).alias('revenue_per_cluster')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding user Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we want to add user information. Please read the json file user.json and name the dataframe user_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<FILL-IN> = spark.read.json(<FILL-IN>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join the predictions and user_df dataframe over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = predictions.join(user_df.withColumnRenamed('Id', 'customerId'), on='customerId')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataframe is not that large anymore. Hence, we can transform it to Pandas by using the method **toPandas()**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pandas = df.select('customerId', 'age','cluster').<FILL-IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the memory usage of the pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check memory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the age distribution of the different clusters and describe the plot.\n",
    "\n",
    "Hint: You can use the boxplot function of the seaborn module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This is the end of the exercise!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark 2.3",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
