{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Models for Titanic Dataset\n",
    "In this exercise we are going to build a classification model which allows us to predict if a passenger on the titanic has survived.\n",
    "\n",
    "As always, **import** the two standard libraries **pandas** as pd and **numpy** as np. Afterwards, **load** the training and test datasets of the titanic data which we have prepared and saved as pickle files during the last exercise. Call the dataframes **train** and **test**. Use the info method on both dataframes.\n",
    "\n",
    "**Hint:** Use the pandas method **read_pickle()** to read the two files **'titanic_train.pkl'** and **'titanic_test.pkl'**. These files are stored under the path **../data/**.\n",
    "\n",
    "**Remark:** If you cannot load the pickle files, please use the csv files which are stored in the samle folder. Please set the optional argument **index_col** to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas and numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read pickle files\n",
    "train = pd.read_pickle(<FILL-IN>)\n",
    "test = <FILL-IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to **separate** the **features** and the classification **target**. Hence, **create** the variables **X_train**, **y_train**, **X_test** and **y_test** from the two dataframes **train** and **test**, where for instance X_train contains all the features and y_train contains only the target **'Survived'** of the dataframe train (analogue for the test data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X_train, X_test = train.drop(['Survived'], axis=<FILL-IN>), test.drop([<FILL-IN>], axis=1)\n",
    "y_train, y_test = train['Survived'], test[<FILL-IN>]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree for Classification\n",
    "\n",
    "The first model we want to train is a single decision tree for classification. Therefore, **import** the class **DecisionTreeClassifier** from the module **sklearn.tree**. Afterwards, **create an object** of that class called **tree_clf**. Pass the argument **random_state=42** to the constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Decision Tree\n",
    "from sklearn.tree import <FILL-IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instance\n",
    "<FILL-IN> = DecisionTreeClassifier(random_state=<FILL-IN>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can fit the model by using the **fit(X,y)** method of the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "tree_clf.fit(<FILL-IN>, <FILL-IN>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "Next, we want to compute the accuracy of our predictions, which is given by the number of correct predictions, divided by all predictions:  \n",
    "\n",
    "$$\\mathrm{Acc}= \\frac{\\# correct\\ predictions}{\\# all\\ samples}$$\n",
    "\n",
    "Therefore, **create** a results dataframe called **results**, which has one column **containing y_test** and another column **containing our predictions** on the test data. To create the predictions you can use the **predict(X)** method of the model on the dataframe **X_test**. Call the resulting numpy array **y_pred**. Afterwards, you can create a dataframe by using the dictionary approach. Call the column holding the true values **y_true**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create predictions\n",
    "y_pred = tree_clf.predict(<FILL-IN>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create result dataframe\n",
    "results = pd.DataFrame({<FILL-IN>: y_test, 'y_pred': <FILL-IN>})\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, **compute the accuarcy** using the results dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute accuracy by hand\n",
    "acc = len(results[<FILL-IN> == <FILL-IN>]) / len(<FILL-IN>)\n",
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, we do not have to compute the accuracy manually, but it is a good practice to compute a performance measure once *by hand*. Please, **crosscheck your result** by using the **accuracy_score** function from the modul **sklearn.metrics**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import accuracy_score\n",
    "from <FILL-IN> import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also another option to compute the accuracy. Most of the sklearn models have a method called **score()**. In the case of classification this score is by default the accuracy. In regression models it is the r2_score. Please use that method to compute the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use model score method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an **imbalanced dataset**, i.e. the classes are not equally distributed, the **accuracy is not a good performance measure**. Can you **explain** why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "Next, we look at another way to evaluate the performance of our classification model: the **confusion matrix**. It is a specific table layout which **holds** the number of True Positives (**TP**), True Negatives (**TN**), False Positives (**FP**) and False Negatives (**FN**).\n",
    "\n",
    "**Bonus:**\n",
    "\n",
    "Please, **compute TP, TN, FP and FN** with the dataframe results. Afterwards, we create the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute TP, TN, FP and FN\n",
    "TP = len(results[(results['y_pred'] == <FILL-IN>) & (results['y_true'] == <FILL-IN>)])\n",
    "TN = len(results[(results['y_pred'] == <FILL-IN>) & (results['y_true'] == <FILL-IN>)])\n",
    "FP = len(results[(results['y_pred'] == <FILL-IN>) & (results['y_true'] == <FILL-IN>)])\n",
    "FN = len(results[(results['y_pred'] == <FILL-IN>) & (results['y_true'] == <FILL-IN>)])\n",
    "\n",
    "# Create confusion matrix\n",
    "C = pd.DataFrame(np.array([[TN, FP], [FN, TP]]))\n",
    "C.index.name = 'true'\n",
    "C.columns.name = 'predicted'\n",
    "C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, there is a function which computes the confusion matrix automatically, given y_test and y_pred. Please, **import** the function **confustion_matrix** from the module **sklearn.metrics** and **crosscheck** your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import conf function\n",
    "from sklearn.metrics import <FILL-IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "C = confusion_matrix(<FILL-IN>, <FILL-IN>)\n",
    "C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision and Recall\n",
    "Next, we want to compute the Precision, given by\n",
    "$$\\mathrm{Precision} = \\frac{TP}{TP + FP} = \\frac{TP}{P^*}$$\n",
    "and the Recall, defined as\n",
    "$$\\mathrm{Recall} = \\frac{TP}{TP + FN} = \\frac{TP}{P},$$  \n",
    "\n",
    "where $P^*$ and $P$ are the numbers of predicted positive instances and of all positive instances, respectively.\n",
    "\n",
    "The Precison describes how precise a positive prediction of the model is. For Instance, if the precision is 0.9 it means that 90% of the positive predictions really belong to the positive class.  \n",
    "Recall (also known as hit rate) describes the proportion of the positive class which has been detected by the model. For Instance, a recall of 0.6 means that 60 % of all positive instances have been detected by the model as such.\n",
    "\n",
    "**Bonus**:\n",
    "\n",
    "Please try to compute the precison and recall if you have computed the values for TP, FP, etc. in the previous exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Recall\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, there are again prebuilt functions to compute these measures. Please **crosscheck** your results by using the functions **precision_score** and **recall_score** from the module **sklearn.metrics**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import functions\n",
    "from <FILL-IN> import precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# precision\n",
    "precision_score(<FILL-IN>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall\n",
    "<FILL-IN>(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Report\n",
    "\n",
    "Most of the performance measures that we have computed so far are contained in the **classification report**. It can be computed by using the method **classification_report** from the module **sklearn.metrics**. Please, use that function and have a look at the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import function\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute report\n",
    "report = classification_report(<FILL-IN>)\n",
    "\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of the Classification Tree\n",
    "Next, we want to visualize two classification trees, similarly to the regression tree in the regression exercise. However, this time we visualize the fully grown tree as well as a pruned tree.\n",
    "\n",
    "Please, **create** a tree object called **tree_2_clf** with **max_depth=2** and **random_state=42** and **train it**. Afterwards, **compute the classification report**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tree with fixed depth of 2\n",
    "<FILL-IN> = DecisionTreeClassifier(max_depth=<FILL-IN>, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "y_pred = tree_2_clf.predict(<FILL-IN>)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hey, the pruned tree seems to be better. Can you explain why?   \n",
    "\n",
    "To visualize both trees we need to **import** the function **export_graphviz** from the module **sklearn.tree**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "from sklearn.tree import <FILL-IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just execute\n",
    "export_graphviz(tree_clf, out_file=\"tree_clf.dot\",\n",
    "                feature_names=X_train.columns.tolist(),\n",
    "                filled=True, rounded=True)\n",
    "\n",
    "export_graphviz(tree_2_clf, out_file=\"tree_2_clf.dot\",\n",
    "                feature_names=X_train.columns.tolist(),\n",
    "                filled=True, rounded=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please, open a terminal and execute the command **sudo apt install graphviz** if you haven't installed that package in the previous exercise. If the package has been installed, you can execute the command in the cell below. Afterwards, you should find the two trees in your execerise folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "dot -Tpng tree_clf.dot -o tree_clf.png\n",
    "dot -Tpng tree_2_clf.dot -o tree_2_clf.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the two trees and try to explain them.   \n",
    "\n",
    "**Bonus**:\n",
    "\n",
    "Try to recompute the first two levels of the small tree. The Gini Impurity is defined as \n",
    "\n",
    "$$ G = \\sum_k \\hat{p}_{mk} (1 - \\hat{p}_{mk})$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\\hat{p}_{mk} = \\frac{1}{N_m} \\sum_{x_i \\in R_m} I(y_i = k)$$\n",
    "\n",
    "is the proportion of training observations in the m-th region from the k-th class. For a binary classifier this can be simplified to \n",
    "\n",
    "$$G = \\hat{p}_{m} (1-\\hat{p}_m) + \\hat{q}_m (1-\\hat{q}_m) = 2 \\hat{p}_m (1-\\hat{p}_m)$$ and \n",
    "\n",
    "$$\\hat{p}_{m} = \\frac{1}{N_m} \\sum_{x_i \\in R_m} I(y_i = 1).$$\n",
    "\n",
    "Additionally, compute the values of $p_m$ for the leafs by using the values given in the figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# level 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# level 1 left\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# level 1 right\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computation of the probabilities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability/Certainty and Precision-Recall Trade-Off\n",
    "\n",
    "The values for p that we have computed in the bonus exercise can be considered as the probability/certainty of a sample in that region/hyperrectangle belonging to class 1. However, this only makes sense for pruned trees. **Try to explain why!**\n",
    "\n",
    "We can get these probabilities by using the method **predict_proba(X)** of the decision tree **tree_2_clf**. Please, compute these probabilities on the test set. Afterwards, **create** a **dataframe** called **results_proba** which consists of a column with the true values **y_test** called **y_true** and another column with the predicted **probabilites for class 1** called **y_proba**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute probabilities\n",
    "y_proba = tree_2_clf.<FILL-IN>(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract proba for class 1\n",
    "<FILL-IN> = y_proba[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe\n",
    "results_proba = pd.DataFrame({'y_true': <FILL-IN>, 'y_proba': y_proba_class1})\n",
    "results_proba.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction Threshold\n",
    "Now, let us introduce the **threshold**. Per default a binary classifier, which can compute probabilities, classifies a sample belonging to class 1 if the probability is **larger than 0.5**. This value is called the threshold. Of course, we can also use different thresholds. \n",
    "\n",
    "**Bonus:**\n",
    "\n",
    "**Compute the classification report for three different thresholds**:\n",
    "\n",
    "a) 0.20  \n",
    "b) 0.50  \n",
    "c) 0.70   \n",
    "\n",
    "**Hint:** Add four new columns called **y_pred_A**, **y_pred_B**, etc. to the dataframe results_proba. Fill them with the value 1 if the probability is larger than the threshold. You can use the numpy method **np.where()**.\n",
    "\n",
    "Do you see any difference in the classification reports?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute predictions for different thresholds\n",
    "results_proba['y_pred_A'] = np.where(results_proba['y_proba'] > 0.2, 1, 0)\n",
    "results_proba['y_pred_B'] = np.where(<FILL-IN>)\n",
    "results_proba['y_pred_C'] = <FILL-IN>\n",
    "results_proba.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute classification reports\n",
    "report_A = classification_report(y_test, results_proba['y_pred_A'])\n",
    "report_B = classification_report(y_test, <FILL-IN>)\n",
    "report_C = classification_report(y_test, <FILL-IN>)\n",
    "print(report_A)\n",
    "print(report_B)\n",
    "print(report_C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe you have noticed that the precision for class 1 increases, while increasing the threshold. However, for the recall we observe the opposite effect. This is called the Precision-Recall Trade-Off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier\n",
    "\n",
    "In this part of the exercise we want to train a random forest classifier. Therefore, please **import** the class **RandomForestClassifier** from the module **sklearn.ensemble**, **create** an **object** of that class called **rf_clf** and **train the model** with the training data **X_train, y_train**. Afterwards, compute the classification report. Please, pass the argument **random_state=42** and **n_estimators=100** to the constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import rf\n",
    "from sklearn.ensemble import <FILL-IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create object\n",
    "<FILL-IN> = RandomForestClassifier(random_state=42, n_estimators=<FILL-IN>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train rf\n",
    "rf_clf.fit(<FILL-IN>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute predictions y_pred\n",
    "y_pred = rf_clf.predict(<FILL-IN>) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute classification report\n",
    "print(classification_report(<FILL-IN>))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the result is much better than for a single tree.   \n",
    "\n",
    "We can also use the random forest to get probabilities with the predict_proba method. However, this time it is computed in a different way compared to a single decision tree. Here, it is the proportion of trees which voted for class 1.\n",
    "\n",
    "Please, **create** a dataframe called **results_rf** which contains **y_test** and the **probabilities for class 1**. Call the two columns **'y_true'** and **'y_proba'**, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute probas\n",
    "y_proba = rf_clf.<FILL-IN>(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe\n",
    "results_rf = pd.DataFrame({'y_true': <FILL-IN>, 'y_proba':y_proba[:,1]})\n",
    "results_rf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bonus:**  \n",
    "\n",
    "**Create a boxplot** which shows the **probability distribution y_proba** for the true values of class 0 and class 1.\n",
    "First, **import seasborn as sns** and issue the command **%matplotlib inline**. Afterwards, use the boxplot method where **x='y_true'** and **y='y_proba'**. For a good classifier both distributions have only a small overlap with each other and the medians should be well separated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import seaborn\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplot\n",
    "sns.boxplot(<FILL-IN>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision/Recall curves\n",
    "Next, we want to investigate the precision-recall trade-off of our model.\n",
    "Sklearn provides a method which computes precisions and recalls for various threshold values. Therefore, import the function **precision_recall_curve** from the module **sklearn.metrics**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import precison_recall_curve\n",
    "from sklearn.metrics import <FILL-IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look at the docstring of the function you notice that this function returns three arrays. We can directly *unpack* the return values into three arrays called precision, recall and threshold. Please, use that function to compute the three numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute precision, recall and threshold\n",
    "precision, recall, threshold = precision_recall_curve(<FILL-IN>, <FILL-IN>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can plot two curves:\n",
    "\n",
    "1. precision (recall) vs. threshold\n",
    "2. precision vs. recall\n",
    "\n",
    "Therefore, please execute the cell below which defines two plotting functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just execute\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def plot_precision_recall_vs_threshold(precisions, recalls, thresholds,\n",
    "                                      color=\"g\", labels=('Precision', 'Recall')):\n",
    "    '''Function takes precisions, recalls and thresholds as arguments and generates a plot with two curves.\n",
    "        Optionally you can define the color and the labels.'''\n",
    "    plt.plot(thresholds, precisions[:-1], color + \"--\", label = labels[0])\n",
    "    plt.plot(thresholds, recalls[:-1], color + \"-\", label = labels[1])\n",
    "    plt.xlabel(\"Threshold\")\n",
    "    plt.legend(loc=\"lower left\")\n",
    "\n",
    "def plot_precision_vs_recall(precisions, recalls, color=\"g\", label='model'):\n",
    "    plt.plot(recalls, precisions, color + \"--\", label = label)\n",
    "    plt.xlabel(\"recall\")\n",
    "    plt.ylabel(\"precision\")\n",
    "    plt.legend(loc=\"upper right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the two functions to create the two plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# precision vs recall\n",
    "plot_precision_vs_recall(precision, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# precision (recall) vs. threshold\n",
    "plot_precision_recall_vs_threshold(precision, recall, threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intersection point of the precision and recall curve is called the **breakeven point**.\n",
    "\n",
    "**Question:** What threshold do we need if we want to have a precision of 0.9? What will be the recall value? (read it off the diagram)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution: ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance\n",
    "In this part we compute the feature importance of the random forest. For a classification problem the features are ranked according to their proportion of the reduction of the gini impurity. The computation will be very similar to the feature importance part of the regression exercise. Please perform the following steps:\n",
    "\n",
    "1. Create a **list** of features called **features** by using the **columns attribute** of X_train and the **tolist()** method.\n",
    "2. Create a **list** of the **feature importance** values by accessing the attribute **feature\\_importances\\_** of the random forest. \n",
    "3. Create a **dataframe importance_df** which contains the two lists as columns.\n",
    "4. Sort the dataframe by the feature importance column in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two lists\n",
    "<FILL-IN> = X_train.columns.tolist()\n",
    "importances = <FILL-IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe importance_df\n",
    "importance_df = pd.DataFrame({'features': featues,\n",
    "                              'importances': importances})\\\n",
    "    .sort_values(by=<FILL-IN>, ascending=<FILL-IN>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly, the feature Pclass does not seem to be that important as we migh have thought. Maybe we have been biased by the movie titanic too much? But wait, what about the feature Fare? It is a numeric feature which describes the price of the ticket. This feature is probably correlated with Pclass.\n",
    "\n",
    "**Bonus**\n",
    "\n",
    "To check if Fare depends on Pclass please create a boxplot which shows the distribution of Fare for the different classes. Furthermore, look at the distribution of Fare for the two different target labels.\n",
    "\n",
    "**Remark**: Use the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Fare with respect to Pclasses\n",
    "sns.boxplot(<FILL-IN>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Fare with respect to the target label\n",
    "df = X_train.join(y_train)\n",
    "sns.boxplot(<FILL-IN>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bonus**: Sum all feature importance values which belong to one categorical variable.\n",
    "\n",
    "**Hint**:\n",
    "\n",
    "1. Use the apply method and the split function on the column importance_df['features']\n",
    "2. Use groupby, sum, and sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new column\n",
    "importance_df['features_split'] = importance_df['features'].apply(lambda x: x.split('=')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute sum\n",
    "importance_df.groupby('features_split').sum().sort_values('importances', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K nearest Neighbors Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enough about decison trees. In this part we want to train a K nearest neighbors classification model. Since this model uses a distance metric for training, we need to standardize the data. This can be done by using the class **StandardScaler** from the module **sklearn.preprocessing**. Please import that class and create an object called **scaler**. Use the **fit_transform** method of that object on **X_train** and the **transform** method on **X_test**. Call the resulting numpy arrays **X_trained_scaled_array** and **X_test_scaled_array**, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import StandardScaler\n",
    "from sklearn.preprocessing import <FILL-IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create object scaler\n",
    "<FILL-IN> = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform fit_transform on X_train and transform on X_test\n",
    "X_train_scaled_array = scaler.<FILL-IN>\n",
    "X_test_scaled_array = scaler.<FILL-IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scaled outputs are numpy arrays. Please **extract the column names and the indices** of the train and test data. Afterwards, **create two dataframes** called **X_train_scaled** and **X_test_scaled** holding the scaled training data and scaled test data, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just execute\n",
    "# Extract columns and indices\n",
    "columns = X_train.columns.tolist()\n",
    "train_index = X_train.index.tolist()\n",
    "test_index = X_test.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just execute\n",
    "# Create scaled dataframes\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled_array,\n",
    "                              columns=columns, index=train_index)\n",
    "\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled_array,\n",
    "                              columns=columns, index=test_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the describe method on both dataframes and check whether scaling was successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check scaling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train our K nearest neighbor classifier. **Import** the class **KNeighborsClassifier** from the module **sklearn.neighbors**. Instanciate an **object** called **knn_clf**. Do not pass any parameters to the constructor. However, you can **check the default values**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import KNeighborsClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create object knn_clf\n",
    "<FILL-IN> = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, train the model on the scaled data, compute the predictions y_pred and the probabilities y_proba for class 1. Call the new created predictions y_pred_knn_scaled and y_pred_probas_class1_knn_scaled. Afterwards, compute the classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just execute\n",
    "# Fit, predict, predict_probas and create classification report\n",
    "knn_clf.fit(X_train_scaled,<FILL-IN>)\n",
    "y_pred_knn_scaled = knn_clf.predict(<FILL-IN>)\n",
    "y_probas_knn_scaled = knn_clf.predict_proba(<FILL-IN>)\n",
    "y_probas_class1_knn_scaled = y_probas_knn_scaled[<FILL-IN>]\n",
    "\n",
    "print(classification_report(y_test, y_pred_knn_scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check whether the scaling was really necessary, please perform the steps above also for the unscaled data X_train and X_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit, predict, predict_probas and create classification report for unscaled data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hey, scaling was worth it! In the theory part we have already learned why it was necessary.\n",
    "\n",
    "During the Feature Engineering part we learned another important part about the usage of ordinal categorical variables. The variable Pclass could be problematic. Do you have an idea why?   \n",
    "\n",
    "\n",
    "We need to *one hot encode* the categorical variable Pclass to use it with a model which uses distance metrics. Please execute the code below which performs the dummy encoding on that column and rejoins it to the training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just execute\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just execute\n",
    "Pclass_train = pd.get_dummies(X_train['Pclass'],\n",
    "               prefix_sep='=',\n",
    "               drop_first=True,\n",
    "               prefix='Pclass')\n",
    "\n",
    "Pclass_test = pd.get_dummies(X_test['Pclass'],\n",
    "               prefix_sep='=',\n",
    "               drop_first=True,\n",
    "               prefix='Pclass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just execute\n",
    "X_train_new = X_train.join(Pclass_train).drop(['Pclass'], axis=1)\n",
    "X_test_new = X_test.join(Pclass_test).drop(['Pclass'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just execute\n",
    "columns = X_train_new.columns.tolist()\n",
    "train_index = X_train_new.index.tolist()\n",
    "test_index = X_test_new.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just execute\n",
    "X_train_scaled_array = scaler.fit_transform(X_train_new)\n",
    "X_test_scaled_array = scaler.transform(X_test_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just execute\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled_array,\n",
    "                              columns=columns, index=train_index)\n",
    "\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled_array,\n",
    "                              columns=columns, index=test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just execute\n",
    "knn_clf.fit(X_train_scaled, y_train)\n",
    "y_pred = knn_clf.predict(X_test_scaled)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is now slightly better. Actually, we should not always modify the model and investigate if the performance on the test dataset has been improved. In this way we somehow try to **fit the test data** (overfitting).\n",
    "\n",
    "**Bonus**:\n",
    "\n",
    "### Cross Validation\n",
    "\n",
    "We can compute a **better estimate for the accuracy of our model on unseen data** by using K-Fold cross validation. Therefore, the **data is split into K folds**. The model is trained K times on K-1 splits of the training data and evaluated on the hold out set (which changes for each fit). Afterwards, we take the mean of all K performances.   \n",
    "\n",
    "Let's **compute an example** of 10-Fold cross validation **for our models** knn_clf, rf_clf, tree_clf and tree_2_clf. First, **import** the function **cross_val_score** from the module **sklearn.model_selection**. Afterwards, **use** the function **cross_val_score** and pass as the **arguments** the **estimator/model**, the **scaled training data X_train_scaled**, our **target y_train**, the number of folds (**cv=10**) and the scoring (**'accuracy'**). The results will be numpy arrays containing 10 scores. Please name the numpy arrays **scores_knn**, **scores_rf**, **scores_tree** and **scores_fixed_tree** for the different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import cross_val_score\n",
    "from sklearn.model_selection import <FILL-IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute 10 Fold Cross validation scores\n",
    "scores_knn = cross_val_score(estimator=<FILL-IN>, X=X_train_scaled, y=y_train, cv=10, scoring='accuracy')\n",
    "scores_rf = <FILL-IN>\n",
    "scores_tree = <FILL-IN>\n",
    "scores_fixed_tree = <FILL-IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have computed the four numpy arrays you can just execute the cell below to compute the mean and the standard deviation of the scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just execute\n",
    "# Print performance results\n",
    "print(\"knn: \\t score = {} +- {}\".format(np.mean(scores_knn), np.std(scores_knn)))\n",
    "print(\"tree: \\t score = {} +- {}\".format(np.mean(scores_tree), np.std(scores_tree)))\n",
    "print(\"tree2: \\t score = {} +- {}\".format(np.mean(scores_fixed_tree), np.std(scores_fixed_tree)))\n",
    "print(\"rf: \\t score = {} +- {}\".format(np.mean(scores_rf), np.std(scores_rf)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hey, it looks like the random forest performs best, closely followed by our k nearest neighbor model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis and Dimensional Reduction\n",
    "\n",
    "In the final part of the exercise we want to reduce the number of dimensions by using a PCA. Afterwards, we fit our K nearest neighbor model (knn) to the data of the first two principal components. Since only two dimensions are left, we can produce a scatter plot to visualize our results. Please follow the follwowing steps:\n",
    "\n",
    "1. Import the class PCA from the module sklearn.decomposition.\n",
    "2. create an object of the PCA class called pca\n",
    "3. fit the object by using its fit method on the training dataset X_train_scaled\n",
    "4. use the transform method of the fitted pca object on X_train_scaled and X_train_test\n",
    "5. call the resulting numpy arrays of step 4 X_train_pca and X_test_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create object pca\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit pca\n",
    "pca.fit(<FILL-IN>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform data\n",
    "X_train_pca = pca.transform(<FILL-IN>)\n",
    "X_test_pca = <FILL-IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the explained variance of the principal components. Please execute the two cells below to create a scree plot and a cumulative variance plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scree plot\n",
    "exp_variance = pca.explained_variance_ratio_\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xticks(np.arange(0, 19))\n",
    "ax.set_xlabel('pca component')\n",
    "ax.set_ylabel('variance ratio')\n",
    "ax.plot(exp_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cumulative variance plot\n",
    "exp_variance = pca.explained_variance_ratio_\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xticks(np.arange(0, 19))\n",
    "ax.set_xlabel('pca component')\n",
    "ax.set_ylabel('cumulative variance ratio')\n",
    "ax.plot(np.cumsum(exp_variance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the cross validation score by using only the first two principal components and the k nearest neighbors classifier knn_clf. Afterwards, compute the mean and the standard deviation.\n",
    "\n",
    "**Hint:** Use slicing to extract the first two principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross_val_score\n",
    "scores_knn_pca = cross_val_score(estimator=knn_clf, X=X_train_pca[:,0:2], y=y_train, cv=10, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean and std\n",
    "print(\"knn_pca: \\t score = {} +- {}\".format(np.mean(scores_knn_pca), np.std(scores_knn_pca)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, this is not that bad for only taking into account less than 30% variance of the data.\n",
    "\n",
    "Finally, we want to visualize the decision boundary in the space of the first two principal components. Hence, install the package mlxtend via the shell command *conda install -c conda-forge mlxtend*. You can also issue the command by using the magic function %%bash. Afterwards, import the function plot_decision_regions from the module mlxtend.plotting and use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import plot_decision_regions\n",
    "from mlxtend.plotting import <FILL-IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_regions(<FILL-IN>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please describe the plot. Does it look like you have overfitted? What parameter should you change?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This is the end of the exercise**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bonus**:   \n",
    "\n",
    "In this bonus exercise we want to find the best value for k, i.e. the best number of nearest neighbors.\n",
    "\n",
    "To find the best value for k we use a **GridSearch** with **cross validation**. In a GridSearch we compute for each combination of hyperparameters (here for each k) a performance score (here accuracy) by using cross validation. For K-Fold cross validation the dataset is split into K-Folds. Afterwards, the model is trained on K-1 splits and evaluated on an hold out set. This is done K times, so that each Fold has been used as a hold out set. \n",
    "\n",
    "Please import the classes **GridSearchCV** and **Kfold** from the **modul sklearn.model_selection**. Create an object of the class KFold called cv. Pass the arguments random_state=42 and shuffle=True to the constructor. Afterwards, we create a python dictionary which we use as a parameter grid. Hence, create a dictionary called param_grid holding only one Key-Value pair. As the key use the string 'n_neighbors' and as the value use a numpy array holding integer values from 1 to 20.\n",
    "\n",
    "**Remark**: You can use the **get_params()** method on any machine learning object to investigate all possible hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import GridSeach and KFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create KFold object (uses 3 splits as default)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check params using get_params()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create parameter grid (dictionary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can create a gridsearch object called grid_clf of the class GridSearchCV. As arguments please use estimator=knn_clf, param_grid=param_grid, scoring='accuracy' and cv=cv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create GridSearch object\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This object can be used like any ordinary classification object/model. Please train the model by passing the first two principal components of X_train_pca and y_train to the fit(X,y) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the gridsearch model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fitted gridsearch estimator grid_clf has several attributes. For instance you can access the best estimator (best_estimator\\_) or best parameters (best_params_)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access best params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, this estimator has the attribute cv\\_results\\_ which gives you a summary dictionary of the gridsearch. You can pass it directly to the pd.DataFrame() method in order to create a dataframe out of it. Please do so and sort the dataframe by rank_test_score in ascending oder.\n",
    "\n",
    "**Remark**: Ignore the warnings :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cv results dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot decision boundaries and test set\n",
    "plot_decision_regions(X_test_pca[:,0:2], y_test.values, grid_clf.fit(X_train_pca[:,0:2], y_train))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
