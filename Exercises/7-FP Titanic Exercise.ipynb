{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering and Preparation for Titanic\n",
    "In one of the previous exercises we have explored the titanic dataset. Now, we want to extract new features from the dataset and make it ready for classification algorithms.\n",
    "\n",
    "As always, we start by importing the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "# Change default figure and font size for plots\n",
    "matplotlib.rcParams['figure.figsize'] = (12.0, 9.0)\n",
    "matplotlib.rcParams.update({'font.size': 12})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please, load the titanic data from the path **'../data/titanic_new.csv'** as a pandas dataframe called **titanic** and use the head and the info method on the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# head\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start dropping columns, filling null values and encoding categorical variables we want to **construct some new features.**\n",
    "\n",
    "Therefore, have a look at the **'name'** column. What kind of feature could we extract from that column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate the name column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Title\n",
    "Maybe the title in front of the first name could be useful? Therefore, let's extract it by using a small python function and regex expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just execute\n",
    "import re\n",
    "def getTitle(name):\n",
    "    'Extracts the word on front of a dot (.)'\n",
    "    title = re.search(r'(\\w+\\.)',name)\n",
    "    return title.group(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, use the **getTitle** function in an **apply method** on the column **'Name'** of the dataframe in order to get a new column containing all the titles. Name this new column **'Title'**.\n",
    "\n",
    "**Hint**: Use a Lambda function in the apply method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create title column\n",
    "titanic[<FILL-IN>] = <FILL-IN>.apply(<FILL-IN>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract all unique titles by using the **unique()** method on the new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique titles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the titles seem to be very uncommon, e.g. *Jonkheer*. If you are interested in what this title means you can have a look at the Wikipedia link (https://en.wikipedia.org/wiki/Jonkheer). There is only one person in the dataset having this title, who is that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Jonkheer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, **check the cardinality** of the different titles by using the method **value_counts** or visualize them by using the seaborn method **sns.countplot(data, column)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check cardinality\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some tiltes are underrepresented. Hence, we combine all uncommon titles to a **new feature** called **'rareTitle'**.\n",
    "This can easily be done by using the map function and a python dictionary. In each Title field it replaces the key with the corresponding value. If the value in the column is not contained in the dictionary keys a null value will be inserted which we directly fill with the value 'rareTitle'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just execute\n",
    "map_dict = {\n",
    "    'Ms.': 'Miss.',\n",
    "    'Mlle.': 'Miss.',\n",
    "    'Mr.' : 'Mr.',\n",
    "    'Mrs.': 'Mrs.',\n",
    "    'Miss.': 'Miss.'\n",
    "}\n",
    "\n",
    "titanic['Title'] = titanic['Title'].map(map_dict).fillna('rareTitle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, check the unique titles and the cardinality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check titles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check cardinality\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we combine the common titles like Miss and Mrs, we can extract another feature: married and unmarried female. Similar as before, we use a map function to generate this new feature.\n",
    "\n",
    "Please add a **new column** called **'marital_status'** and apply a **map function** using the map_dict on the 'Title' column. Afterwards, **fill the null values** with the string **'Unknown'**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new feature marital_status\n",
    "map_dict = {\n",
    "    'Miss.' : 'no',\n",
    "    'Mrs.': 'yes',\n",
    "}\n",
    "\n",
    "<FILL-IN> = <FILL-IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the unique elements and the cardinality of the new feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cardinality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we **combine the common titles** in the 'Title' columns to the category **'noTitle'**. Therefore, we use the numpy method **np.where()**. Check the docstring to see how it works.\n",
    "\n",
    "Create a boolean Pandas Series which contains the value False if the title is common and True if it is rare. Call this series cond."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create boolean series\n",
    "<FILL-IN> = <FILL-IN>\n",
    "\n",
    "# Perform conditional replacement\n",
    "titanic['Title'] = np.where(cond, 'rareTitle', 'noTitle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, check the **cardinality** of the **Title** feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cardinality\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deck\n",
    "We can extract another feature from the 'Cabin' column which by itself is not doing much. A lot of null values are contained in that column since only 1st class passengers have cabins. A **cabin number** looks like **'C123'**, where the **first letter refers to the deck**. We extract the deck in a similar fashion as the title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just execute\n",
    "\n",
    "# Cabin list\n",
    "cabin_list = ['A', 'B', 'C', 'D', 'E', 'F', 'T', 'G']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just execute\n",
    "\n",
    "# Extract function\n",
    "def charInString(charlist, text):\n",
    "    for element in charlist:\n",
    "        if element in text:\n",
    "            return element\n",
    "    return 'Unknown'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the above function on two different cabin numbers, e.g. 'C123' and 'X555'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, apply the new function to the column **'Cabin'** by using the **apply** method and add the result to the titanic dataframe as a new **column** called **'Deck'**. Notice, first you have to fill ne null values of that column. Therefore, you can use the fillna method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature Deck\n",
    "<FILL > = <FILL-IN>.fillna(<FILL-IN>).apply(<FILL-IN>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, check the unique values and the cardinality of the new feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cardinality\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Family Size\n",
    "Finally, we add a new feature **Family_Size**, which is just the sum of SibSp (number of siblings/spouses aboard) and Parch (number of parents/children aboard).\n",
    "\n",
    "Please create this new column called **Familiy_Size**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column Familiy_Size\n",
    "<Fill-IN> = <FILL-IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can prepare our data so that we can feed it to a classification model in the next exercise. This part will be very similar to the feature preparation part of the house pricing dataset, i.e. splitting the data into a train and test dataset, filling null values and encoding the catogrical variables to numerical ones.\n",
    "\n",
    "First, **drop** the unnecessary columns **'Cabin'**, **'PassengerId'**, **'Name'**, ***'SibSp'**, **'Parch'** and **'Ticket'**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary cols\n",
    "titanic.drop(<FILL-IN>, axis=<FILL-IN>, errors='ignore', inplace=True)\n",
    "\n",
    "titanic.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since all the categorical features have no null values, we can already 'dummy encode' them. Therefore, use the Pandas method **get_dummies** with the arguments data=titanic, prefix_sep='=' and drop_first=True. Call the resulting dataframe **titanic_dum** and investigate the new dataframe by using the **info** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot Encoding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only remaining column which contains null values is the **'Age'** column. Instead of just filling the values by the median or mean we design our **own imputation method**. Maybe you remember, that the **medians differed across the passenger classes**. Hence, we fill each null value with the median of the class group. In Sklearn it is straight forward to construct custom preprocessing functions and classes. Therefore, we build a custom class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just execute\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "\n",
    "class AgeImputer(TransformerMixin, BaseEstimator):\n",
    "    '''Custom Imputator which computes the median of the age column grouped \n",
    "    by another feature and fills the null values accordingly'''\n",
    "    \n",
    "    def __init__(self, col, copy=True):\n",
    "        self.col = col # col to groupyBy\n",
    "        self.copy = copy # option\n",
    "        self.median = {} # medianDict\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        # fitting procedure fills the median dict\n",
    "        self.median = X[['Age', self.col]].groupby(self.col).median().to_dict()['Age']\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_ = X if not self.copy else X.copy()\n",
    "        for key in self.median:\n",
    "            # filling NaN values using conditional Expressions\n",
    "            X_.loc[(  (X_['Age'].isnull()) & (X_['Pclass'] == key)),'Age'] = self.median[key]\n",
    "        return X_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class above contains a fit method. As we already know we are **only allowed to use fit functions** on the training set. Hence, we have to **split** our data into **test** and **training datasets**.\n",
    "\n",
    "Please, **import** the **train_test_split** function from the modul **sklearn.model_selection** and apply the function to the dataframe **titanic_dum**. Set the **test_size** to **0.2** and the **random_state** argument to **42**. Call the resulting dataframes **titanic_train** and **titanic_test**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataframes in train and test\n",
    "titanic_train, <FILL-IN> = <FILL-IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use our custom AgeImputer.\n",
    "\n",
    "Create an instance of that class called **ageImputer**. Set the argument **col to 'Pclass'**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instance ageImputer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, use the **fit_transform** method on the **titanic_train** dataset and afterwards the **transform** method on the **titanic_test** dataset. Call the results **train_df** and **test_df**, respectively. Finally, use the **info** methods on both results to check if all null values are gone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill Nulls\n",
    "<FILL-IN> = <FILL-IN>\n",
    "test_df = <FILL-IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check test df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check train df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, save the two dataframes as python binary pickle files. Therefore, use the method to_pickle on the two dataframes. Call the files *titanic_train.pkl* and *titanic_test.pkl*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframes as pickle file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This is the end of this exercise.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
